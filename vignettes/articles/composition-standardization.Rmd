---
title: "Age or length composition standardization with sdmTMB"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: '../refs.bib'
csl: '../mee.csl'
vignette: >
  %\VignetteIndexEntry{Age or length composition standardization with sdmTMB}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

**If the code in this vignette has not been evaluated, a rendered version is available on the [documentation site](https://pbs-assess.github.io/sdmTMB/index.html) under 'Articles'.**

```{r setup, include = FALSE, cache=FALSE}
dplyr_installed <- require("dplyr", quietly = TRUE)
ggplot_installed <- require("ggplot2", quietly = TRUE)
pkgs <- dplyr_installed && ggplot_installed
EVAL <- identical(Sys.getenv("NOT_CRAN"), "true") && pkgs
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.asp = 0.618,
  eval = EVAL,
  purl = EVAL
)
```

```{r, message=FALSE, warning=FALSE}
library(sdmTMB)
library(dplyr)
library(ggplot2)
theme_set(theme_light())
```

Read in Dover Sole length composition data for SYN WCVI survey:

```{r}
dat <- sdmTMB::add_utm_columns(dover_lengths, c("lon", "lat"))
```

Look at the distribution of lengths:

```{r}
range(dat$length)
hist(dat$length)
```

Decide on some bins. I'm aiming to have adequate data in each bin while encompassing all data.

```{r}
bins <- c(0, seq(25, 50, 5), 80)
dat$length_bin <- factor(bins[findInterval(dat$length, bins)])
```

Check what that looks like:

```{r}
ggplot(dat, aes(lon, lat)) +
  geom_point(aes(colour = length)) +
  facet_grid(length_bin ~ year) +
  scale_colour_viridis_c()
```

And make sure we have data in each bin. If not, we'd either have to adjust our bin structure or map off the estimation of some year-bin combinations...

```{r}
table(dat$year, dat$length_bin)
```

Now condense our data into counts per bin-year combination:

```{r}
dg <- group_by(dat, year, trawl_id, X, Y, length_bin) |>
  summarise(n = n(), .groups = "drop")

ggplot(dg, aes(X, Y)) +
  geom_point(aes(colour = n)) +
  facet_grid(length_bin ~ year) +
  scale_colour_viridis_c()
```

Make our SPDE mesh:

```{r}
mesh <- make_mesh(dg, c("X", "Y"), cutoff = 10)
plot(mesh)
mesh$mesh$n
```

Now we have to make a decision about what random field variances we want to share vs. allow to vary.

Here, I'm going to estimate a separate SD for the spatiotemporal fields within each bin.

We'll also have an overall spatial field.

For this spatiotemporal-bin random field variance structure, we need a different factor level for each column in our spatially varying coefficient model matrix for a given length bin.

The following code is me working it out for this case. It should be the same as long as you set up your spatially varying formula the same way with `~ 0 + factor(year) * factor(bin)`.

```{r}
na <- colnames(model.matrix(~ 0 + factor(year) * length_bin, data = dg))
na

nbins <- length(unique(dg$length_bin))
nbins
nyears <- length(unique(dg$year))
nyears

svc_map <- c(rep(1, nyears), 2:nbins, rep(2:nbins, each = nyears - 1))
svc_map <- factor(svc_map)
svc_map
```

Check that this makes sense:

```{r}
length(svc_map)
length(na)
data.frame(na, svc_map)
```

Now, fit our model. We'll use `profile = TRUE`, which is likely faster here because of all the fixed effects (moves the fixed effects into the inner optimization with the random effects).

```{r, results="hide", message=FALSE, warning=FALSE}
# dg$id <- factor(seq(1, nrow(dg))) # for lognormal Poisson

fit <- sdmTMB(
  n ~ factor(year) * length_bin, # + (1 | id)
  mesh = mesh,
  data = dg,
  family = poisson(), # consider NB2 or lognormal Poisson
  time = "year",
  silent = FALSE,
  spatial = "on",
  spatiotemporal = "off", # year fields are in the SVC component below
  spatial_varying = ~ 0 + factor(year) * length_bin,
  control = sdmTMBcontrol(map = list(ln_tau_Z = svc_map), profile = TRUE),
)
```

```{r}
sanity(fit)
summary(fit)
```

If we have estimation issues on the SDs of some bins/categories then we may need to share some of the SDs. Adjust the map vector as needed. May expand vignette to show that...

```{r}
set.seed(1)
r <- residuals(fit, type = "mle-mvn")
qqnorm(r)
abline(0, 1)
```

A little off in the upper tails with the Poisson. Could try NB2 or lognormal Poisson. 

We could also try modelling weight instead of count.

get abundance density indexes for each year-bin combination:

```{r}
nd <- replicate_df(sdmTMB::wcvi_grid, "year", unique(dg$year))
bins <- sort(unique(dg$length_bin))
```

Loop over the bins and calculate an index for each:

```{r, results='hide', warning=FALSE, message=FALSE}
ind_list <- lapply(bins, \(b) {
  cat("Calculating index for bin:", as.character(b), "\n")
  nd$length_bin <- b
  pred <- predict(fit, newdata = nd, return_tmb_object = TRUE)
  # base_correct = FALSE here just for vignette building speed
  ind <- get_index(pred, area = 4, bias_correct = FALSE)
  data.frame(ind, bin = b)
})
ind <- do.call(rbind, ind_list)
```

Here is our index by bin:

```{r}
head(ind)
ggplot(ind, aes(year, est)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr)) +
  geom_line(colour = "white") +
  facet_wrap(~bin, scales = "fixed") +
  scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, 0.05)))
```

We need a little function to approximate the standard error for the proportions and the effective sample sizes. For now, I've written the following. It awkwardly lifts some code from VAST for and then reshapes it back into a data frame. Something better should be in sdmTMB eventually...

```{r}
get_comp_neff <- function(dat, index_df, time_column = "year", bin_column = "length_bin") {
  nyrs <- length(unique(dat[[time_column]]))
  nbins <- length(unique(dat[[bin_column]]))

  x <- t(matrix(index_df$est, nrow = nyrs, ncol = nbins))
  Index_ctl <- array(x, dim = c(nbins, nyrs, 1))
  x <- t(matrix(index_df$se_natural, nrow = nyrs, ncol = nbins))
  SE_Index_ctl <- array(x, dim = c(nbins, nyrs, 1))

  ## From VAST:
  # Calculate proportions, and total biomass
  Prop_ctl <- Index_ctl / outer(
    rep(1, dim(Index_ctl)[1]),
    apply(Index_ctl, MARGIN = 2:3, FUN = sum)
  )
  Index_tl <- apply(Index_ctl, MARGIN = 2:3, FUN = sum)
  SE_Index_tl <- sqrt(apply(SE_Index_ctl^2, MARGIN = 2:3, FUN = sum, na.rm = TRUE))

  # Approximate variance for proportions, and effective sample size
  Neff_ctl <- var_Prop_ctl <- array(NA, dim = dim(Prop_ctl))
  for (cI in 1:dim(var_Prop_ctl)[1]) {
    for (tI in 1:dim(var_Prop_ctl)[2]) {
      for (lI in 1:dim(var_Prop_ctl)[3]) {
        var_Prop_ctl[cI, tI, lI] <- Index_ctl[cI, tI, lI]^2 / Index_tl[tI, lI]^2 *
          (SE_Index_ctl[cI, tI, lI]^2 / Index_ctl[cI, tI, lI]^2 -
            2 * SE_Index_ctl[cI, tI, lI]^2 / (Index_ctl[cI, tI, lI] *
              Index_tl[tI, lI]) + SE_Index_tl[tI, lI]^2 / Index_tl[tI, lI]^2)
        var_Prop_ctl[cI, tI, lI] <-
          ifelse(Index_ctl[cI, tI, lI] == 0, 0, var_Prop_ctl[cI, tI, lI]) # If dividing by zero, replace with 0
        # Convert to effective sample size
        Neff_ctl[cI, tI, lI] <- Prop_ctl[cI, tI, lI] * (1 - Prop_ctl[cI, tI, lI]) / var_Prop_ctl[cI, tI, lI]
      }
    }
  }

  # Median effective sample size across categories
  Neff_tl <- apply(Neff_ctl, MARGIN = 2:3, FUN = median, na.rm = TRUE)
  ## End from VAST

  out <- reshape2::melt(var_Prop_ctl) |>
    select(-Var3) |>
    rename(bin_i = Var1, year_i = Var2) |>
    mutate(prop_se = sqrt(value)) |>
    select(-value)
  out2 <- reshape2::melt(Prop_ctl) |>
    select(-Var3) |>
    rename(bin_i = Var1, year_i = Var2) |>
    mutate(prop = value) |>
    select(-value)
  out3 <- reshape2::melt(Neff_ctl) |>
    select(-Var3) |>
    rename(bin_i = Var1, year_i = Var2) |>
    mutate(Neff = value) |>
    select(-value)
  out4 <- reshape2::melt(Neff_tl) |>
    select(-Var2) |>
    rename(year_i = Var1, Neff_median = value)

  out <- left_join(out, out2, by = join_by(bin_i, year_i)) |>
    left_join(out3, by = join_by(bin_i, year_i)) |>
    left_join(out4, by = join_by(year_i))
  luy <- dat[, time_column, drop = FALSE] |>
    distinct() |>
    mutate(year_i = seq(1, n()))
  lua <- dat[, bin_column, drop = FALSE] |>
    distinct() |>
    mutate(bin_i = seq(1, n()))
  left_join(out, luy, by = join_by(year_i)) |>
    left_join(lua, join_by(bin_i)) |>
    select(-bin_i, -year_i)
}
```

Now use our function:

```{r}
out <- get_comp_neff(dg, ind, "year", "length_bin")
head(out)
```

And visualize the output. Here I'm plotting +/- 1 SE.

```{r}
ggplot(out, aes(as.numeric(length_bin), prop)) +
  facet_wrap(~year) +
  geom_linerange(aes(ymin = prop - 1 * prop_se, ymax = prop + 1 * prop_se)) +
  geom_line() +
  coord_cartesian(ylim = c(0, NA)) +
  ylab("Proportion") + xlab("Length bin")
```

These are our median effective sample sizes by year across bins:

```{r}
group_by(out, year) |>
  summarise(neff_median = Neff_median[1])
```
