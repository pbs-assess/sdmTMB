---
title: "Age or length composition standardization with sdmTMB"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: '../refs.bib'
csl: '../mee.csl'
vignette: >
  %\VignetteIndexEntry{Age or length composition standardization with sdmTMB}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

**If the code in this vignette has not been evaluated, a rendered version is available on the [documentation site](https://pbs-assess.github.io/sdmTMB/index.html) under 'Articles'.**

```{r setup, include = FALSE, cache=FALSE}
dplyr_installed <- require("dplyr", quietly = TRUE)
ggplot_installed <- require("ggplot2", quietly = TRUE)
pkgs <- dplyr_installed && ggplot_installed
EVAL <- identical(Sys.getenv("NOT_CRAN"), "true") && pkgs
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.asp = 0.618,
  eval = EVAL,
  purl = EVAL
)
```

```{r, message=FALSE, warning=FALSE}
library(sdmTMB)
library(dplyr)
library(ggplot2)
theme_set(theme_light())
```

```{r, eval=FALSE, echo=FALSE}
if (FALSE) {

}
```

Read in Dover Sole length composition data for SYN WCVI survey

```{r}
d <- readRDS("scratch/dover_lengths.rds")
d <- sdmTMB::add_utm_columns(d, c("lon", "lat"))
```

Look at the distribution of lengths

```{r}
range(d$length)
hist(d$length)
```

Decide on some bins. I'm aiming to have adequate data in each bin while encompassing all data.

```{r}
bins <- c(0, seq(25, 50, 5), 80)
d$length_bin <- factor(bins[findInterval(d$length, bins)])
```

Check what that looks like

```{r}
ggplot(d, aes(lon, lat)) +
  geom_point(aes(colour = length)) +
  facet_grid(length_bin ~ year) +
  scale_colour_viridis_c()
```

And make sure we have data in each bin. If not, we'd either have to adjust our bin structure or map off the estimation of some year-bin combinations...

```{r}
table(d$year, d$length_bin)
```

Now condense our data into counts per bin-year combination

```{r}
dg <- group_by(d, year, trawl_id, X, Y, length_bin) |>
  summarise(n = n(), .groups = "drop")

ggplot(dg, aes(X, Y)) +
  geom_point(aes(colour = n)) +
  facet_grid(length_bin ~ year) +
  scale_colour_viridis_c()
```

Make our SPDE mesh

```{r}
mesh <- make_mesh(dg, c("X", "Y"), cutoff = 8)
plot(mesh)
mesh$mesh$n
```

we now have a decision to make about what random field variances we want to share vs. allow to vary

here, I'm going to estimate a separate SD for the spatiotemporal fields within each bin

we'll also have an overall spatial field

for this spatiotemporal-bin random field variance structure, we need a different factor level for each column in our spatially varying coefficient model matrix for a given length bin

this gets a little crazy... sorry... maybe we can work out a function to automate this

```{r}
na <- colnames(model.matrix(~ 0 + factor(year) * length_bin, data = dg))
na

nbins <- length(unique(dg$length_bin))
nbins
nyears <- length(unique(dg$year))
nyears

svc_map <- c(rep(1, nyears), 2:nbins, rep(2:nbins, each = nyears - 1))
svc_map <- factor(svc_map)
svc_map
```

check this makes sense:

```{r}
length(svc_map)
length(na)
data.frame(na, svc_map)
```

fit our model... `profile = TRUE` is likely faster here because of all the fixed effects (moves the fixed effects into the inner optimization with the random effects)

```{r, results="hide", message=FALSE, warning=FALSE}
# dg$id <- factor(seq(1, nrow(dg))) # for lognormal Poisson

fit <- sdmTMB(
  n ~ factor(year) * length_bin, # + (1 | id)
  mesh = mesh,
  data = dg,
  family = poisson(), # consider NB2 or lognormal Poisson
  time = "year",
  silent = FALSE,
  spatial = "on",
  spatiotemporal = "off", # year fields are in the SVC component below
  spatial_varying = ~ 0 + factor(year) * length_bin,
  control = sdmTMBcontrol(map = list(ln_tau_Z = svc_map), profile = TRUE),
)
```

```{r}
sanity(fit)
summary(fit)
```

We we have estimation issues on the SDs of some bins/categories, we may need to share some of the SDs. Adjust the map vector as needed.

```{r}
set.seed(1)
r <- residuals(fit, type = "mle-mvn")
qqnorm(r)
abline(0, 1)
```

A little off in the upper tails with the Poisson. Could try NB2 or lognormal Poisson. 

We could also try modelling weight instead of count.

get abundance density indexes for each year-bin combination:

```{r}
nd <- replicate_df(sdmTMB::wcvi_grid, "year", unique(dg$year))
bins <- sort(unique(dg$length_bin))
```

loop over ages and calculate index for each:

```{r}
ind_list <- lapply(bins, \(b) {
  cat("Calculating index for bin:", as.character(b), "\n")
  nd$length_bin <- b
  pred <- predict(fit, newdata = nd, return_tmb_object = TRUE)
  ind <- get_index(pred, area = 4, bias_correct = TRUE)
  data.frame(ind, bin = b)
})
ind <- do.call(rbind, ind_list)
```

here is our index by bin

```{r}
ggplot(ind, aes(year, est)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr)) +
  geom_line(colour = "white") +
  facet_wrap(~bin, scales = "fixed") +
  scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, 0.05)))
```

We need a little function to approximate the standard error for the proportions and the effective sample sizes. For now, I've written the following. It awkwardly lifts some code from VAST for and then reshapes it back into a data frame. Something better should be in sdmTMB eventually...

```{r}
get_comp_neff <- function(dat, index_df, time_column = "year", bin_column = "length_bin") {
  nyrs <- length(unique(dat[[time_column]]))
  nbins <- length(unique(dat[[bin_column]]))

  x <- t(matrix(index_df$est, nrow = nyrs, ncol = nbins))
  Index_ctl <- array(x, dim = c(nbins, nyrs, 1))
  x <- t(matrix(index_df$se_natural, nrow = nyrs, ncol = nbins))
  SE_Index_ctl <- array(x, dim = c(nbins, nyrs, 1))

  ## From VAST:
  # Calculate proportions, and total biomass
  Prop_ctl <- Index_ctl / outer(
    rep(1, dim(Index_ctl)[1]),
    apply(Index_ctl, MARGIN = 2:3, FUN = sum)
  )
  Index_tl <- apply(Index_ctl, MARGIN = 2:3, FUN = sum)
  SE_Index_tl <- sqrt(apply(SE_Index_ctl^2, MARGIN = 2:3, FUN = sum, na.rm = TRUE))

  # Approximate variance for proportions, and effective sample size
  Neff_ctl <- var_Prop_ctl <- array(NA, dim = dim(Prop_ctl))
  for (cI in 1:dim(var_Prop_ctl)[1]) {
    for (tI in 1:dim(var_Prop_ctl)[2]) {
      for (lI in 1:dim(var_Prop_ctl)[3]) {
        var_Prop_ctl[cI, tI, lI] <- Index_ctl[cI, tI, lI]^2 / Index_tl[tI, lI]^2 *
          (SE_Index_ctl[cI, tI, lI]^2 / Index_ctl[cI, tI, lI]^2 -
            2 * SE_Index_ctl[cI, tI, lI]^2 / (Index_ctl[cI, tI, lI] *
              Index_tl[tI, lI]) + SE_Index_tl[tI, lI]^2 / Index_tl[tI, lI]^2)
        var_Prop_ctl[cI, tI, lI] <-
          ifelse(Index_ctl[cI, tI, lI] == 0, 0, var_Prop_ctl[cI, tI, lI]) # If dividing by zero, replace with 0
        # Convert to effective sample size
        Neff_ctl[cI, tI, lI] <- Prop_ctl[cI, tI, lI] * (1 - Prop_ctl[cI, tI, lI]) / var_Prop_ctl[cI, tI, lI]
      }
    }
  }

  # Median effective sample size across categories
  Neff_tl <- apply(Neff_ctl, MARGIN = 2:3, FUN = median, na.rm = TRUE)
  ## End from VAST

  out <- reshape2::melt(var_Prop_ctl) |>
    select(-Var3) |>
    rename(bin_i = Var1, year_i = Var2) |>
    mutate(prop_se = sqrt(value)) |>
    select(-value)
  out2 <- reshape2::melt(Prop_ctl) |>
    select(-Var3) |>
    rename(bin_i = Var1, year_i = Var2) |>
    mutate(prop = value) |>
    select(-value)
  out3 <- reshape2::melt(Neff_ctl) |>
    select(-Var3) |>
    rename(bin_i = Var1, year_i = Var2) |>
    mutate(Neff = value) |>
    select(-value)
  out4 <- reshape2::melt(Neff_tl) |>
    select(-Var2) |>
    rename(year_i = Var1, Neff_median = value)

  out <- left_join(out, out2, by = join_by(bin_i, year_i)) |>
    left_join(out3, by = join_by(bin_i, year_i)) |>
    left_join(out4, by = join_by(year_i))
  luy <- dat[, time_column, drop = FALSE] |>
    distinct() |>
    mutate(year_i = seq(1, n()))
  lua <- dat[, bin_column, drop = FALSE] |>
    distinct() |>
    mutate(bin_i = seq(1, n()))
  left_join(out, luy, by = join_by(year_i)) |>
    left_join(lua, join_by(bin_i)) |>
    select(-bin_i, -year_i)
}
```

Now use our function:

```{r}
out <- get_comp_neff(dg, ind, "year", "length_bin")
head(out)
```

And visualize the output

```{r}
ggplot(out, aes(as.numeric(length_bin), prop)) +
  facet_wrap(~year) +
  geom_linerange(aes(ymin = prop - .5 * prop_se, ymax = prop + .5 * prop_se)) +
  geom_line() +
  scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, 0.04)))

ggplot(out, aes(as.numeric(length_bin), prop, colour = factor(year))) +
  geom_linerange(aes(ymin = prop - .5 * prop_se, ymax = prop + .5 * prop_se), position = position_dodge(width = 0.2)) +
  geom_line(position = position_dodge(width = 0.2)) +
  scale_colour_viridis_d() +
  scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, 0.04)))
```

These are our median effective sample sizes by year across bins:

```{r}
group_by(out, year) |>
  summarise(neff_median = Neff_median[1])
```
